{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaf03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a59030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1a34c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [3, 5, 10],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(probability=True),\n",
    "        'params': {\n",
    "            'C': [0.1, 1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb1d0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Logistic Regression...\n",
      "Tuning Decision Tree...\n",
      "Tuning Random Forest...\n",
      "Tuning SVM...\n",
      "Tuning Naive Bayes...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for name, mp in model_params.items():\n",
    "    print(f\"Tuning {name}...\")\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', mp['model'])\n",
    "    ])\n",
    "    \n",
    "    grid = GridSearchCV(pipe, {'clf__' + k: v for k, v in mp['params'].items()},\n",
    "                        cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Best Params': grid.best_params_,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': auc\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42250b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Leaderboard:\n",
      "                 Model  F1-Score   AUC-ROC  Overall Rank\n",
      "0  Logistic Regression  0.979310  0.995701             1\n",
      "3                  SVM  0.986111  0.993717             1\n",
      "2        Random Forest  0.958333  0.992560             3\n",
      "4          Naive Bayes  0.944444  0.986772             4\n",
      "1        Decision Tree  0.928571  0.909226             5\n"
     ]
    }
   ],
   "source": [
    "leaderboard = pd.DataFrame(results)\n",
    "leaderboard['F1-Rank'] = leaderboard['F1-Score'].rank(ascending=False).astype(int)\n",
    "leaderboard['AUC-Rank'] = leaderboard['AUC-ROC'].rank(ascending=False).astype(int)\n",
    "leaderboard['Overall Rank'] = (leaderboard['F1-Rank'] + leaderboard['AUC-Rank']).rank().astype(int)\n",
    "\n",
    "leaderboard = leaderboard.sort_values('Overall Rank')\n",
    "print(\"\\n Final Leaderboard:\")\n",
    "print(leaderboard[['Model', 'F1-Score', 'AUC-ROC', 'Overall Rank']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
